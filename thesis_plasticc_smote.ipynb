{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "rn.seed(10)\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(10)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "flat_recall_results = []\n",
    "flat_precision_results=[]\n",
    "flat_accuracy_results=[]\n",
    "flat_balanced_accuracy=[]\n",
    "flat_f1_val=[]\n",
    "\n",
    "\n",
    "stack_recall_results = []\n",
    "stack_precision_results=[]\n",
    "stack_accuracy_results=[]\n",
    "stack_balanced_accuracy=[]\n",
    "stack_f1_val=[]\n",
    "\n",
    "\n",
    "ancestor_recall_results = []\n",
    "ancestor_precision_results=[]\n",
    "ancestor_accuracy_results=[]\n",
    "ancestor_balanced_accuracy=[]\n",
    "ancestor_f1_val=[]\n",
    "\n",
    "\n",
    "parent_recall_results = []\n",
    "parent_precision_results=[]\n",
    "parent_accuracy_results=[]\n",
    "parent_balanced_accuracy=[]\n",
    "parent_f1_val=[]\n",
    "\n",
    "target_recall_results = []\n",
    "target_precision_results=[]\n",
    "target_accuracy_results=[]\n",
    "target_balanced_accuracy=[]\n",
    "target_f1_val=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plastic_data = pd.read_csv('C:/kv/Thesus/PLAStiCC/plasticc_filtered_data.csv',header=0)\n",
    "plastic_data = pd.read_csv('C:/kv/Thesus/PLAStiCC/plasticc_non_filtered_data.csv',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_pla_data.columns[full_pla_data.isnull().any()]\n",
    "plastic_data.columns[plastic_data.isnull().any()]\n",
    "plastic_data = plastic_data.fillna(value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plastic_data.info()\n",
    "#plastic_data = plastic_data.fillna(value=0)\n",
    "\n",
    "\n",
    "#plastic_data = full_pla_data.fillna(value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4381, 197)\n",
      "(4381, 3)\n",
      "(array([42, 52, 62, 67, 90], dtype=int64), array([1193,  183,  484,  208, 2313], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "X = plastic_data[plastic_data.columns.difference(['object_id','target','parent_label','ancestor_label'])]\n",
    "y = plastic_data[['target','parent_label','ancestor_label']]\n",
    "\n",
    "X=X.astype(float)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(np.unique(y.target,return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not required now\n",
    "def flat_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=197, activation='softplus',kernel_initializer=glorot_normal(seed=50)))\n",
    "    model.add(Dense(102,activation='softplus'))\n",
    "\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def stack_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(205, input_dim=202, activation='softplus',kernel_initializer=glorot_normal(seed=50)))\n",
    "    model.add(Dense(105,activation='softplus'))\n",
    "\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def ancestor_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=197, activation='softplus',kernel_initializer=glorot_normal(seed=50)))\n",
    "   \n",
    "    model.add(Dense(101,activation='softplus')) #12 10 8 \n",
    "\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def parent_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=199, activation='softplus',kernel_initializer=glorot_normal(seed=50)))\n",
    "\n",
    "    model.add(Dense(102,activation='softplus')) #12 60 bal\n",
    "\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    optimizer = Adagrad(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def target_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(205, input_dim=202, activation='softplus',kernel_initializer=glorot_normal(seed=50)))\n",
    "    model.add(Dense(105,activation='softplus'))\n",
    "\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "from keras.layers import Input,Dense,Dropout\n",
    "from keras.models import Model,Sequential\n",
    "from keras.initializers import glorot_normal,glorot_uniform\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.optimizers import Adam,Adagrad\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "def deep_flat_model():\n",
    "    model = Sequential() #200 150 100 75 25 5\n",
    "    model.add(Dense(200, input_dim=197, activation='softplus',kernel_initializer=glorot_normal(seed=100)))\n",
    "    model.add(Dense(175,activation='softplus'))\n",
    "    model.add(Dense(150,activation='softplus'))\n",
    "    model.add(Dense(125,activation='softplus'))\n",
    "    model.add(Dense(100,activation='softplus'))\n",
    "    model.add(Dense(75,activation='softplus'))\n",
    "    model.add(Dense(50,activation='softplus'))\n",
    "    model.add(Dense(25,activation='softplus'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def deep_stack_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(205, input_dim=202, activation='softplus',kernel_initializer=glorot_normal(seed=100)))\n",
    "    model.add(Dense(175,activation='softplus'))\n",
    "    model.add(Dense(150,activation='softplus'))\n",
    "    model.add(Dense(125,activation='softplus'))\n",
    "    model.add(Dense(100,activation='softplus'))\n",
    "    model.add(Dense(75,activation='softplus'))\n",
    "    model.add(Dense(50,activation='softplus'))\n",
    "    model.add(Dense(25,activation='softplus'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def deep_ancestor_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=197, activation='softplus',kernel_initializer=glorot_normal(seed=100)))\n",
    "    model.add(Dense(175,activation='softplus'))\n",
    "    model.add(Dense(150,activation='softplus'))\n",
    "    model.add(Dense(125,activation='softplus'))\n",
    "    model.add(Dense(100,activation='softplus'))\n",
    "    model.add(Dense(75,activation='softplus'))\n",
    "    model.add(Dense(50,activation='softplus'))\n",
    "    model.add(Dense(25,activation='softplus'))\n",
    "\n",
    "\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def deep_parent_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=199, activation='softplus',kernel_initializer=glorot_normal(seed=100)))\n",
    "    \n",
    "    model.add(Dense(175,activation='softplus'))\n",
    "    model.add(Dense(150,activation='softplus'))\n",
    "    model.add(Dense(125,activation='softplus'))\n",
    "    model.add(Dense(100,activation='softplus'))\n",
    "    model.add(Dense(75,activation='softplus'))\n",
    "    model.add(Dense(50,activation='softplus'))\n",
    "    model.add(Dense(25,activation='softplus'))\n",
    "\n",
    "\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    optimizer = Adagrad(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def deep_target_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(205, input_dim=202, activation='softplus',kernel_initializer=glorot_normal(seed=100)))\n",
    "    model.add(Dense(175,activation='softplus'))\n",
    "    model.add(Dense(150,activation='softplus'))\n",
    "    model.add(Dense(125,activation='softplus'))\n",
    "    model.add(Dense(100,activation='softplus'))\n",
    "    model.add(Dense(75,activation='softplus'))\n",
    "    model.add(Dense(50,activation='softplus'))\n",
    "    model.add(Dense(25,activation='softplus'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def getTrainTestData(X,y,random_seed):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=random_seed)\n",
    "    X_train=X_train.reset_index(drop=True)\n",
    "    X_test =X_test.reset_index(drop=True)\n",
    "    y_train=y_train.reset_index(drop=True)\n",
    "    y_test =y_test.reset_index(drop=True)\n",
    "    \n",
    "    return X_train,X_test,y_train,y_test\n",
    "\n",
    "def performScaling(X_train,X_test,y_train,y_test,col_list):\n",
    "\n",
    "    from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n",
    "    scaler = RobustScaler()\n",
    "   \n",
    "    X_train_scl = scaler.fit_transform(X_train) #X_train #X_train_res\n",
    "    X_test_scl = scaler.transform(X_test)\n",
    "\n",
    "    X_train_scl = pd.DataFrame(X_train_scl, columns = col_list) #X-train_scl\n",
    "    X_test_scl  = pd.DataFrame(X_test_scl, columns = col_list)\n",
    "\n",
    "    y_train=y_train.reset_index(drop=True)\n",
    "    y_test =y_test.reset_index(drop=True)\n",
    "\n",
    "    scaled_train_df = pd.concat([X_train_scl,y_train],axis=1)\n",
    "    scaled_test_df = pd.concat([X_test_scl,y_test],axis=1)\n",
    "    \n",
    "    return X_train_scl,X_test_scl,y_train,y_test\n",
    "\n",
    "\n",
    "def performSMOTE(X_train,X_test,y_train,y_test,col_list):\n",
    "   \n",
    "\n",
    "    print(\"Before OverSampling, counts of label '42': {}\".format(sum(y_train.target==42))) #sum(hy_train==0)\n",
    "    print(\"Before OverSampling, counts of label '52': {}\".format(sum(y_train.target==52)))\n",
    "\n",
    "    print(\"Before OverSampling, counts of label '62': {} \\n\".format(sum(y_train.target==62)))\n",
    "    print(\"Before OverSampling, counts of label '67': {} \\n\".format(sum(y_train.target==67)))\n",
    "    print(\"Before OverSampling, counts of label '90': {} \\n\".format(sum(y_train.target==90)))\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "    print(\"Applying SMOTE\")\n",
    "\n",
    "    from imblearn.over_sampling import SMOTE, ADASYN\n",
    "    sm = SMOTE(random_state=2)\n",
    "    X_train_res, y_train_res = sm.fit_sample(X_train, y_train.target.ravel())\n",
    "\n",
    "    #ad = ADASYN(sampling_strategy='auto',random_state=2)\n",
    "    #X_train_res, y_train_res = ad.fit_resample(X_train, y_train.sub_label.ravel())\n",
    "\n",
    "    print(\"After OverSampling, counts of label '42': {}\".format(sum(y_train_res==42))) #sum(hy_train==0)\n",
    "    \n",
    "    print(\"After OverSampling, counts of label '52': {}\".format(sum(y_train_res==52)))\n",
    "\n",
    "    print(\"After OverSampling, counts of label '62': {} \\n\".format(sum(y_train_res==62)))\n",
    "    print(\"After OverSampling, counts of label '67': {} \\n\".format(sum(y_train_res==67)))\n",
    "    print(\"After OverSampling, counts of label '90': {} \\n\".format(sum(y_train_res==90)))\n",
    "\n",
    "\n",
    "\n",
    "    #print(\" train\", np.unique(y_train_res,return_counts=True))\n",
    "    \n",
    "    X_train_res = pd.DataFrame(X_train_res, columns=col_list)\n",
    "    y_train_res = pd.DataFrame(y_train_res, columns=['target'])\n",
    "\n",
    "    y_train_res['parent_label']=y_train_res.target\n",
    "    y_train_res['ancestor_label']=y_train_res.target\n",
    "    \n",
    "    \n",
    "    \n",
    "    y_train_res.loc[y_train_res.target==67,['parent_label']]=90\n",
    "    y_train_res.loc[y_train_res.target==52,['parent_label']]=90\n",
    "    \n",
    "    \n",
    "    y_train_res.loc[y_train_res.parent_label==90,['ancestor_label']]=100\n",
    "    y_train_res.loc[y_train_res.parent_label==62,['ancestor_label']]=100\n",
    "    y_train_res.loc[y_train_res.parent_label==42,['ancestor_label']]=42\n",
    "    \n",
    "    \n",
    "    ##y_train_res=y_train_res.reset_index(drop=True)\n",
    "    #y_test =y_test.reset_index(drop=True)\n",
    "\n",
    "    smote_train_df = pd.concat([X_train_res,y_train_res],axis=1)\n",
    "    smote_test_df = pd.concat([X_test,y_test],axis=1)\n",
    "    \n",
    "    #print(\" test\", np.unique(y_test,return_counts=True))\n",
    "\n",
    "    return smote_train_df,smote_test_df\n",
    "\n",
    "def get_estimator(model,X,y,weights):\n",
    "    estimator = train_model(model,X,y)\n",
    "    estimator.fit(X,y,class_weight=weights)\n",
    " \n",
    "    return estimator\n",
    "\n",
    "def train_model(base,X,y):\n",
    "    \n",
    "    from sklearn.model_selection import StratifiedKFold,cross_val_score  \n",
    "    estimator = KerasClassifier(build_fn=base, epochs=200, batch_size=100,verbose=0)\n",
    "   \n",
    "        \n",
    "    #kfold = StratifiedKFold(n_splits=10, shuffle=True,random_state=100)#random_state=0\n",
    "    #results = cross_val_score(estimator,X,y, cv=kfold,scoring='balanced_accuracy')                              \n",
    "    #print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))  \n",
    "    print(\"skipped cv\")\n",
    "    return estimator\n",
    "\n",
    "def evaluate_model(classifier,X,y):\n",
    "   \n",
    "    \n",
    "    predictions,predict_prob,accuracy,bal_accuracy,precision,recall,f1,confusion_mat = get_metrics(classifier,X,y)\n",
    "    \n",
    "    print_metrics(accuracy,bal_accuracy,precision,recall,f1,confusion_mat)\n",
    "    \n",
    "    return predictions,predict_prob,accuracy,bal_accuracy,precision,recall,f1,confusion_mat\n",
    "\n",
    "def get_metrics(classifier,X,y):\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,balanced_accuracy_score,f1_score\n",
    "    predictions,predict_prob = get_predictions(classifier,X)\n",
    "    accuracy = accuracy_score(y,predictions)\n",
    "    balanced_accuracy= balanced_accuracy_score(y,predictions)\n",
    "    precision=precision_score(y,predictions,average=None)\n",
    "    recall = recall_score(y,predictions,average=None)\n",
    "    confusion_mat = confusion_matrix(y,predictions)\n",
    "    f1 = f1_score(y, predictions, labels=None, pos_label=1,average=None, sample_weight=None)\n",
    "    \n",
    "    return predictions,predict_prob,accuracy,balanced_accuracy,precision,recall,f1,confusion_mat\n",
    "\n",
    "def get_predictions(classifier,X):\n",
    "    return classifier.predict(X), classifier.predict_proba(X)\n",
    "\n",
    "\n",
    "def print_metrics(accuracy,balanced_accuracy,precision,recall,f1,confusion_mat):    \n",
    "    print(\" Accuracy\", accuracy)\n",
    "    print(\" Balanced accuracy\",balanced_accuracy)\n",
    "    print(\" Recall\",recall )\n",
    "    print(\" Precision\",precision)\n",
    "    print(\"f1\",f1)\n",
    "    print(confusion_mat)\n",
    "    \n",
    "    \n",
    "def getExperimentData(train_df,test_df,label_type):\n",
    "    \n",
    "    X_train = train_df[train_df.columns.difference(['ancestor_label','parent_label','target'])]\n",
    "    X_test = test_df[test_df.columns.difference(['ancestor_label','parent_label','target'])]\n",
    "    \n",
    "    if (label_type==\"ancestor\"):\n",
    "        \n",
    "        Y_train = train_df[['ancestor_label']]\n",
    "        Y_test = test_df[['ancestor_label']]\n",
    "    \n",
    "    elif(label_type==\"parent\"):\n",
    "       \n",
    "        Y_train = train_df[['parent_label']]\n",
    "        Y_test = test_df[['parent_label']]\n",
    "    \n",
    "    else:\n",
    "        Y_train = train_df[['target']]\n",
    "        Y_test = test_df[['target']]\n",
    "    \n",
    "    \n",
    "    print(label_type + \"X_train:\", X_train.shape)\n",
    "    print(label_type + \"Y_train:\", Y_train.shape)\n",
    "\n",
    "    print(label_type + \"X_test\", X_test.shape)\n",
    "    print(label_type + \"Y_test\", Y_test.shape)\n",
    "\n",
    "    print(label_type + \"Y_train:\", np.unique(Y_train,return_counts=True))\n",
    "    print(label_type + \"Y_test:\", np.unique( Y_test,return_counts=True))\n",
    "    print(\"\\n \\n\")\n",
    "    \n",
    "    return X_train,X_test,Y_train,Y_test\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_probs(dataset,predict_prob,method):\n",
    "    if method==\"stack\":\n",
    "        ip = pd.DataFrame(predict_prob, columns=['prob1','prob2','prob3','prob4','prob5'])\n",
    "        X = dataset.copy()\n",
    "        X['prob1']=ip[['prob1']]\n",
    "        X['prob2']=ip[['prob2']]\n",
    "        X['prob3']=ip[['prob3']]\n",
    "        X['prob4']=ip[['prob4']]\n",
    "        X['prob5']=ip[['prob5']]\n",
    "        \n",
    "    elif method==\"ancestor\":\n",
    "        ip = pd.DataFrame(predict_prob, columns=['prob1','prob2'])\n",
    "        X = dataset.copy()\n",
    "        X['prob1']=ip[['prob1']]\n",
    "        X['prob2']=ip[['prob2']]\n",
    "    \n",
    "    elif method==\"parent\":\n",
    "        ip = pd.DataFrame(predict_prob, columns=['prob3','prob4','prob5'])\n",
    "        X = dataset.copy()\n",
    "        X['prob3']=ip[['prob3']]\n",
    "        X['prob4']=ip[['prob4']]\n",
    "        X['prob5']=ip[['prob5']]\n",
    "\n",
    "        \n",
    "    return X\n",
    "\n",
    "def put_in_df(metric,hierarchy):\n",
    "    \n",
    "    if (hierarchy==\"accuracy\"):\n",
    "        headers = ['accuracy','bal_accuracy']\n",
    "    elif (hierarchy==\"ancestor\"):\n",
    "        headers = ['class42','class100']\n",
    "    elif (hierarchy==\"parent\"):\n",
    "        \n",
    "        headers = ['class42','class62','class90']\n",
    "        \n",
    "    else:\n",
    "        headers= ['class42','class52','class62','class67','class90']\n",
    "    \n",
    "    df = pd.DataFrame(metric,columns=headers)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_weights(y_train):\n",
    "    \n",
    "    from sklearn.utils import class_weight\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                     np.unique(y_train),\n",
    "                                                     y_train)\n",
    "\n",
    "\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    return class_weight_dict\n",
    "\n",
    "\n",
    "def get_class_weights(y):\n",
    "    from collections import Counter\n",
    "    counter = Counter(y)\n",
    "    majority = max(counter.values())\n",
    "    return  {cls: float(majority/count) for cls, count in counter.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del parent_X_train,parent_X_test,parent_Y_train,parent_Y_test\n",
    "#del target_X_train,target_X_test,target_Y_train,target_Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '42': 885\n",
      "Before OverSampling, counts of label '52': 139\n",
      "Before OverSampling, counts of label '62': 369 \n",
      "\n",
      "Before OverSampling, counts of label '67': 157 \n",
      "\n",
      "Before OverSampling, counts of label '90': 1735 \n",
      "\n",
      "Applying SMOTE\n",
      "After OverSampling, counts of label '42': 1735\n",
      "After OverSampling, counts of label '52': 1735\n",
      "After OverSampling, counts of label '62': 1735 \n",
      "\n",
      "After OverSampling, counts of label '67': 1735 \n",
      "\n",
      "After OverSampling, counts of label '90': 1735 \n",
      "\n",
      "flatX_train: (8675, 197)\n",
      "flatY_train: (8675, 1)\n",
      "flatX_test (1096, 197)\n",
      "flatY_test (1096, 1)\n",
      "flatY_train: (array([42, 52, 62, 67, 90], dtype=int64), array([1735, 1735, 1735, 1735, 1735], dtype=int64))\n",
      "flatY_test: (array([42, 52, 62, 67, 90], dtype=int64), array([308,  44, 115,  51, 578], dtype=int64))\n",
      "\n",
      " \n",
      "\n",
      "ancestorX_train: (8675, 197)\n",
      "ancestorY_train: (8675, 1)\n",
      "ancestorX_test (1096, 197)\n",
      "ancestorY_test (1096, 1)\n",
      "ancestorY_train: (array([ 42, 100], dtype=int64), array([1735, 6940], dtype=int64))\n",
      "ancestorY_test: (array([ 42, 100], dtype=int64), array([308, 788], dtype=int64))\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "col = X.columns\n",
    "X_train,X_test,y_train,y_test = getTrainTestData(X,y,50)\n",
    "\n",
    "X_train_scl,X_test_scl,y_train_scl,y_test_scl = performScaling(X_train,X_test,y_train,y_test,col)\n",
    "scaled_train_df,scaled_test_df = performSMOTE(X_train_scl,X_test_scl,y_train_scl,y_test_scl,col)\n",
    "\n",
    "\n",
    "\n",
    "flat_X_train,flat_X_test,flat_Y_train,flat_Y_test = getExperimentData(scaled_train_df, scaled_test_df,\"flat\")\n",
    "ancestor_X_train,ancestor_X_test,ancestor_Y_train,ancestor_Y_test = getExperimentData(scaled_train_df, scaled_test_df,\"ancestor\")\n",
    "#parent_X_train,parent_X_test,parent_Y_train,parent_Y_test = getExperimentData(scaled_train_df, scaled_test_df,\"parent\")\n",
    "#target_X_train,target_X_test,target_Y_train,target_Y_test = getExperimentData(scaled_train_df, scaled_test_df,\"target\")\n",
    "\n",
    "#del scaled_train_df, scaled_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped cv\n",
      " Accuracy 0.6751824817518248\n",
      " Balanced accuracy 0.5238929126637867\n",
      " Recall [0.58766234 0.34090909 0.55652174 0.33333333 0.80103806]\n",
      " Precision [0.69615385 0.12396694 0.50793651 0.43589744 0.84181818]\n",
      "f1 [0.63732394 0.18181818 0.53112033 0.37777778 0.82092199]\n",
      "[[181  36  37   8  46]\n",
      " [  9  15   3   1  16]\n",
      " [ 13  17  64   8  13]\n",
      " [  8   5   9  17  12]\n",
      " [ 49  48  13   5 463]]\n"
     ]
    }
   ],
   "source": [
    "#weights = get_weights(flat_Y_train.target)\n",
    "weights={}\n",
    "flat_estimator = get_estimator(flat_model,flat_X_train,flat_Y_train,weights)\n",
    "#base_estimator = get_estimator(base_class_model,base_X_train,base_Y_train,50,weights)\n",
    "\n",
    "flat_train_predictions,flat_train_predict_prob = get_predictions(flat_estimator,flat_X_train)\n",
    "\n",
    "flat_predictions,flat_predict_prob,flat_accuracy,flat_bal_accuracy,flat_precision,flat_recall,flat_f1,flat_confusion_mat=evaluate_model(flat_estimator,flat_X_test,flat_Y_test)\n",
    "\n",
    "flat_recall_results.append(flat_recall)\n",
    "flat_precision_results.append(flat_precision)\n",
    "flat_accuracy_results.append([flat_accuracy,flat_bal_accuracy])\n",
    "flat_f1_val.append(flat_f1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7162408759124088, 0.5427001539539946],\n",
       " [0.6879562043795621, 0.5369704135351692],\n",
       " [0.7116788321167883, 0.5385504461221688],\n",
       " [0.7062043795620438, 0.5327657409104914],\n",
       " [0.6751824817518248, 0.5238929126637867]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8675, 202)\n",
      "(1096, 202)\n",
      "(array([42, 52, 62, 67, 90], dtype=int64), array([1735, 1735, 1735, 1735, 1735], dtype=int64))\n",
      "(array([42, 52, 62, 67, 90], dtype=int64), array([308,  44, 115,  51, 578], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "stack_X_train = get_probs(flat_X_train,flat_train_predict_prob,\"stack\")\n",
    "stack_X_test = get_probs(flat_X_test,flat_predict_prob,\"stack\")\n",
    "\n",
    "\n",
    "stack_Y_train= scaled_train_df[['target']]\n",
    "stack_Y_test = scaled_test_df[['target']]\n",
    "\n",
    "\n",
    "\n",
    "print(stack_X_train.shape)\n",
    "print(stack_X_test.shape)\n",
    "print(np.unique(stack_Y_train,return_counts=True))\n",
    "\n",
    "print(np.unique(stack_Y_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped cv\n",
      " Accuracy 0.708941605839416\n",
      " Balanced accuracy 0.5312475699417176\n",
      " Recall [0.73051948 0.25       0.52173913 0.35294118 0.80103806]\n",
      " Precision [0.62154696 0.31428571 0.55045872 0.46153846 0.84029038]\n",
      "f1 [0.67164179 0.27848101 0.53571429 0.4        0.82019486]\n",
      "[[225   2  28   7  46]\n",
      " [ 13  11   4   1  15]\n",
      " [ 25   7  60   8  15]\n",
      " [ 10   3   8  18  12]\n",
      " [ 89  12   9   5 463]]\n"
     ]
    }
   ],
   "source": [
    "#weights=get_weights(stack_Y_train.target)\n",
    "weights={}\n",
    "stack_estimator = get_estimator(stack_model,stack_X_train,stack_Y_train,weights)\n",
    "#base_estimator = get_estimator(base_class_model,base_X_train,base_Y_train,50,weights)\n",
    "\n",
    "stack_train_predictions,stack_train_predict_prob = get_predictions(stack_estimator,stack_X_train)\n",
    "\n",
    "stack_predictions,stack_predict_prob,stack_accuracy,stack_bal_accuracy,stack_precision,stack_recall,stack_f1,stack_confusion_mat=evaluate_model(stack_estimator,stack_X_test,stack_Y_test)\n",
    "\n",
    "stack_recall_results.append(stack_recall)\n",
    "stack_precision_results.append(stack_precision)\n",
    "stack_accuracy_results.append([stack_accuracy,stack_bal_accuracy])\n",
    "stack_f1_val.append(stack_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7208029197080292, 0.535573658067682],\n",
       " [0.7098540145985401, 0.5466543006537176],\n",
       " [0.7135036496350365, 0.5367059025394647],\n",
       " [0.6897810218978102, 0.5321263276228568],\n",
       " [0.708941605839416, 0.5312475699417176]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stack_accuracy_results.pop()\n",
    "#stack_precision_results.pop()\n",
    "#stack_recall_results.pop()\n",
    "#stack_f1_val.pop()\n",
    "stack_accuracy_results\n",
    "\n",
    "#np.unique(ancestor_Y_train,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.64041096, 0.25531915, 0.46875   , 0.42857143, 0.88481675]),\n",
       " array([0.71088435, 0.19148936, 0.60606061, 0.41666667, 0.80817052]),\n",
       " array([0.72258065, 0.22      , 0.52419355, 0.39583333, 0.82092199]),\n",
       " array([0.60507246, 0.14583333, 0.66942149, 0.44230769, 0.79799666]),\n",
       " array([0.73051948, 0.25      , 0.52173913, 0.35294118, 0.80103806])]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_recall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped cv\n",
      " Accuracy 0.8093065693430657\n",
      " Balanced accuracy 0.7303476586470301\n",
      " Recall [0.55932203 0.90137328]\n",
      " Precision [0.67622951 0.84741784]\n",
      "f1 [0.6122449  0.87356322]\n",
      "[[165 130]\n",
      " [ 79 722]]\n"
     ]
    }
   ],
   "source": [
    "#ancestor_Y_train.loc[ancestor_Y_train.ancestor_label==42,['ancestor_label']]=0\n",
    "#ancestor_Y_train.loc[ancestor_Y_train.ancestor_label==100,['ancestor_label']]=1\n",
    "\n",
    "#ancestor_Y_test.loc[ancestor_Y_test.ancestor_label==42,['ancestor_label']]=0\n",
    "#ancestor_Y_test.loc[ancestor_Y_test.ancestor_label==100,['ancestor_label']]=1\n",
    "\n",
    "#weights=get_weights(ancestor_Y_train.ancestor_label)\n",
    "\n",
    "weights={}\n",
    "\n",
    "ancestor_estimator = get_estimator(deep_ancestor_model,ancestor_X_train,ancestor_Y_train,weights)\n",
    "#base_estimator = get_estimator(base_class_model,base_X_train,base_Y_train,50,weights)\n",
    "\n",
    "ancestor_train_predictions,ancestor_train_predict_prob = get_predictions(ancestor_estimator,ancestor_X_train)\n",
    "\n",
    "ancestor_predictions,ancestor_predict_prob,ancestor_accuracy,ancestor_bal_accuracy,ancestor_precision,ancestor_recall,ancestor_f1,ancestor_confusion_mat=evaluate_model(ancestor_estimator,ancestor_X_test,ancestor_Y_test)\n",
    "\n",
    "ancestor_recall_results.append(ancestor_recall)\n",
    "ancestor_precision_results.append(ancestor_precision)\n",
    "ancestor_accuracy_results.append([ancestor_accuracy,ancestor_bal_accuracy])\n",
    "ancestor_f1_val.append(ancestor_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7928832116788321, 0.68632772155611],\n",
       " [0.7956204379562044, 0.6995811855670103],\n",
       " [0.7983576642335767, 0.7160753828433477],\n",
       " [0.8166058394160584, 0.7292610310957302],\n",
       " [0.8093065693430657, 0.7303476586470301]]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#weights=get_weights(ancestor_Y_train.ancestor_label)\n",
    "#weights\n",
    "ancestor_accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights = get_class_weights(ancestor_Y_train.ancestor_label)\n",
    "#weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8730, 199)\n",
      "(1096, 199)\n",
      "(array([42, 62, 90], dtype=int64), array([1746, 1746, 5238], dtype=int64))\n",
      "(array([42, 62, 90], dtype=int64), array([295, 124, 677], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "parent_X_train = get_probs(ancestor_X_train,ancestor_train_predict_prob,\"ancestor\")\n",
    "parent_X_test = get_probs(ancestor_X_test,ancestor_predict_prob,\"ancestor\")\n",
    "\n",
    "\n",
    "parent_Y_train= scaled_train_df[['parent_label']]\n",
    "parent_Y_test = scaled_test_df[['parent_label']]\n",
    "\n",
    "\n",
    "print(parent_X_train.shape)\n",
    "print(parent_X_test.shape)\n",
    "print(np.unique(parent_Y_train,return_counts=True))\n",
    "\n",
    "print(np.unique(parent_Y_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped cv\n",
      " Accuracy 0.7326642335766423\n",
      " Balanced accuracy 0.6276451874028016\n",
      " Recall [0.56949153 0.45967742 0.85376662]\n",
      " Precision [0.66932271 0.48305085 0.79504814]\n",
      "f1 [0.61538462 0.47107438 0.82336182]\n",
      "[[168  25 102]\n",
      " [ 20  57  47]\n",
      " [ 63  36 578]]\n"
     ]
    }
   ],
   "source": [
    "#weights=get_weights(parent_Y_train.parent_label)\n",
    "weights={}\n",
    "parent_estimator = get_estimator(deep_parent_model,parent_X_train,parent_Y_train,weights)\n",
    "#base_estimator = get_estimator(base_class_model,base_X_train,base_Y_train,50,weights)\n",
    "\n",
    "parent_train_predictions,parent_train_predict_prob = get_predictions(parent_estimator,parent_X_train)\n",
    "\n",
    "parent_predictions,parent_predict_prob,parent_accuracy,parent_bal_accuracy,parent_precision,parent_recall,parent_f1,parent_confusion_mat=evaluate_model(parent_estimator,parent_X_test,parent_Y_test)\n",
    "\n",
    "parent_recall_results.append(parent_recall)\n",
    "parent_precision_results.append(parent_precision)\n",
    "parent_accuracy_results.append([parent_accuracy,parent_bal_accuracy])\n",
    "parent_f1_val.append(parent_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7171532846715328, 0.6174234978721611],\n",
       " [0.7463503649635036, 0.6466338217357475],\n",
       " [0.7326642335766423, 0.6500290661981305],\n",
       " [0.7372262773722628, 0.6511305351813738],\n",
       " [0.7326642335766423, 0.6276451874028016]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8730, 202)\n",
      "(1096, 202)\n",
      "(array([42, 52, 62, 67, 90], dtype=int64), array([1746, 1746, 1746, 1746, 1746], dtype=int64))\n",
      "(array([42, 52, 62, 67, 90], dtype=int64), array([295,  42, 124,  68, 567], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "target_X_train = get_probs(parent_X_train,parent_train_predict_prob,\"parent\")\n",
    "target_X_test = get_probs(parent_X_test,parent_predict_prob,\"parent\")\n",
    "\n",
    "target_Y_train= scaled_train_df[['target']]\n",
    "target_Y_test = scaled_test_df[['target']]\n",
    "\n",
    "print(target_X_train.shape)\n",
    "print(target_X_test.shape)\n",
    "print(np.unique(target_Y_train,return_counts=True))\n",
    "print(np.unique(target_Y_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped cv\n",
      " Accuracy 0.6897810218978102\n",
      " Balanced accuracy 0.5095446481854997\n",
      " Recall [0.55254237 0.19047619 0.47580645 0.45588235 0.87301587]\n",
      " Precision [0.68487395 0.28571429 0.48360656 0.57407407 0.75688073]\n",
      "f1 [0.61163227 0.22857143 0.4796748  0.50819672 0.81081081]\n",
      "[[163  10  22   8  92]\n",
      " [  4   8  10   1  19]\n",
      " [ 19   3  59   7  36]\n",
      " [  5   2  18  31  12]\n",
      " [ 47   5  13   7 495]]\n"
     ]
    }
   ],
   "source": [
    "weights={}\n",
    "\n",
    "target_estimator = get_estimator(deep_target_model,target_X_train,target_Y_train,weights)\n",
    "#base_estimator = get_estimator(base_class_model,base_X_train,base_Y_train,50,weights)\n",
    "\n",
    "target_train_predictions,target_train_predict_prob = get_predictions(target_estimator,target_X_train)\n",
    "\n",
    "target_predictions,target_predict_prob,target_accuracy,target_bal_accuracy,target_precision,target_recall,target_f1,target_confusion_mat=evaluate_model(target_estimator,target_X_test,target_Y_test)\n",
    "\n",
    "target_recall_results.append(target_recall)\n",
    "target_precision_results.append(target_precision)\n",
    "target_accuracy_results.append([target_accuracy,target_bal_accuracy])\n",
    "target_f1_val.append(target_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.6998175182481752, 0.5658700159721741],\n",
       " [0.708029197080292, 0.5431598514825764],\n",
       " [0.7153284671532847, 0.5319163046527158],\n",
       " [0.7125912408759124, 0.529763190345413],\n",
       " [0.6943430656934306, 0.5528728095737818]]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_accuracy_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.6368613138686131, 0.5567175350534115],\n",
       " [0.7016423357664233, 0.5366728881241327],\n",
       " [0.718065693430657, 0.5344877332241443],\n",
       " [0.6843065693430657, 0.5351158097053048],\n",
       " [0.7107664233576643, 0.5592622655444941]]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.656934306569343, 0.510078440225562],\n",
       " [0.6578467153284672, 0.5305132436971323],\n",
       " [0.6788321167883211, 0.5014843271707056],\n",
       " [0.6897810218978102, 0.5383243248574731],\n",
       " [0.6897810218978102, 0.5095446481854997]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.57142857, 0.22807018, 0.68041237, 0.76315789, 0.81913043]),\n",
       " array([0.62146893, 0.375     , 0.53153153, 0.5       , 0.83303411]),\n",
       " array([0.60294118, 0.27777778, 0.55445545, 0.625     , 0.83986371]),\n",
       " array([0.63194444, 0.18181818, 0.60638298, 0.60784314, 0.79095164]),\n",
       " array([0.65034965, 0.4375    , 0.56756757, 0.29090909, 0.83662478])]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_precision_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.62408759, 0.10798122, 0.72727273, 0.78787879, 0.84569138]),\n",
       " array([0.62130178, 0.35294118, 0.52252252, 0.48837209, 0.82105263]),\n",
       " array([0.62883436, 0.3030303 , 0.54285714, 0.51282051, 0.83473862]),\n",
       " array([0.65217391, 0.21052632, 0.33488372, 0.625     , 0.86245353]),\n",
       " array([0.58333333, 0.4516129 , 0.57272727, 0.64444444, 0.84181818])]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_precision_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.52702703, 0.2       , 0.57608696, 0.58536585, 0.81226766]),\n",
       " array([0.72815534, 0.23728814, 0.33189655, 0.46341463, 0.82616487]),\n",
       " array([0.55976676, 0.25806452, 0.47321429, 0.47619048, 0.82922535]),\n",
       " array([0.63945578, 0.18367347, 0.51219512, 0.48214286, 0.81707317]),\n",
       " array([0.68487395, 0.28571429, 0.48360656, 0.57407407, 0.75688073])]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_precision_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.64163823, 0.30232558, 0.515625  , 0.55769231, 0.81206897]),\n",
       " array([0.6875    , 0.30769231, 0.48760331, 0.41176471, 0.82123894]),\n",
       " array([0.6996587 , 0.21276596, 0.53333333, 0.39215686, 0.82166667]),\n",
       " array([0.61904762, 0.09090909, 0.4789916 , 0.59615385, 0.8637138 ]),\n",
       " array([0.63050847, 0.33333333, 0.50806452, 0.47058824, 0.82186949])]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_recall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.58361775, 0.53488372, 0.4375    , 0.5       , 0.72758621]),\n",
       " array([0.65625   , 0.30769231, 0.47933884, 0.41176471, 0.82831858]),\n",
       " array([0.6996587 , 0.21276596, 0.54285714, 0.39215686, 0.825     ]),\n",
       " array([0.6122449 , 0.09090909, 0.60504202, 0.57692308, 0.79045997]),\n",
       " array([0.71186441, 0.33333333, 0.50806452, 0.42647059, 0.81657848])]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_recall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.66552901, 0.25581395, 0.4140625 , 0.46153846, 0.75344828]),\n",
       " array([0.46875   , 0.35897436, 0.63636364, 0.37254902, 0.8159292 ]),\n",
       " array([0.6552901 , 0.17021277, 0.5047619 , 0.39215686, 0.785     ]),\n",
       " array([0.63945578, 0.20454545, 0.52941176, 0.51923077, 0.79897785]),\n",
       " array([0.55254237, 0.19047619, 0.47580645, 0.45588235, 0.87301587])]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_recall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.60450161, 0.26      , 0.58666667, 0.64444444, 0.81558442]),\n",
       " array([0.65281899, 0.33802817, 0.50862069, 0.4516129 , 0.82709447]),\n",
       " array([0.64770932, 0.24096386, 0.54368932, 0.48192771, 0.83066554]),\n",
       " array([0.62542955, 0.12121212, 0.53521127, 0.60194175, 0.8257329 ]),\n",
       " array([0.64027539, 0.37837838, 0.53617021, 0.35955056, 0.82918149])]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.6031746 , 0.1796875 , 0.54634146, 0.61176471, 0.78220575]),\n",
       " array([0.63829787, 0.32876712, 0.5       , 0.44680851, 0.8246696 ]),\n",
       " array([0.66235864, 0.25      , 0.54285714, 0.44444444, 0.82984074]),\n",
       " array([0.63157895, 0.12698413, 0.43113772, 0.6       , 0.82488889]),\n",
       " array([0.64122137, 0.38356164, 0.53846154, 0.51327434, 0.82900627])]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.58823529, 0.2244898 , 0.48181818, 0.51612903, 0.78175313]),\n",
       " array([0.57034221, 0.28571429, 0.43626062, 0.41304348, 0.82101514]),\n",
       " array([0.60377358, 0.20512821, 0.48847926, 0.43010753, 0.80650685]),\n",
       " array([0.63945578, 0.19354839, 0.52066116, 0.5       , 0.8079242 ]),\n",
       " array([0.61163227, 0.22857143, 0.4796748 , 0.50819672, 0.81081081])]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parent_acc_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-929b7b9f38d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mstack_precision_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstack_recall_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstack_f1_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mflat_precision_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mflat_recall_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mflat_f1_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mancestor_precision_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mancestor_recall_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mancestor_f1_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparent_precision_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparent_recall_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparent_f1_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_precision_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_recall_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_f1_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mflat_acc_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstack_acc_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mancestor_acc_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparent_acc_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparent_acc_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'parent_acc_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "ancestor_precision_df=put_in_df(ancestor_precision_results,\"ancestor\")\n",
    "ancestor_recall_df=put_in_df(ancestor_recall_results,\"ancestor\")\n",
    "ancestor_f1_df=put_in_df(ancestor_f1_val,\"ancestor\")\n",
    "\n",
    "\n",
    "parent_precision_df=put_in_df(parent_precision_results,\"parent\")\n",
    "parent_recall_df=put_in_df(parent_recall_results,\"parent\")\n",
    "parent_f1_df=put_in_df(parent_f1_val,\"parent\")\n",
    "\n",
    "target_precision_df=put_in_df(target_precision_results,\"target\")\n",
    "target_recall_df=put_in_df(target_recall_results,\"target\")\n",
    "target_f1_df=put_in_df(target_f1_val,\"target\")\n",
    "\n",
    "\n",
    "flat_precision_df=put_in_df(flat_precision_results,\"flat\")\n",
    "flat_recall_df=put_in_df(flat_recall_results,\"flat\")\n",
    "flat_f1_df=put_in_df(flat_f1_val,\"flat\")\n",
    "\n",
    "\n",
    "stack_precision_df=put_in_df(stack_precision_results,\"flat\")\n",
    "stack_recall_df=put_in_df(stack_recall_results,\"flat\")\n",
    "stack_f1_df=put_in_df(stack_f1_val,\"flat\")\n",
    "\n",
    "\n",
    "flat_acc_df = put_in_df(flat_accuracy_results,\"accuracy\")\n",
    "stack_acc_df = put_in_df(stack_accuracy_results,\"accuracy\")\n",
    "ancestor_acc_df = put_in_df(ancestor_accuracy_results,\"accuracy\")\n",
    "parent_acc_df = put_in_df(parent_accuracy_results,\"accuracy\")\n",
    "target_acc_df = put_in_df(target_accuracy_results,\"accuracy\")\n",
    "\n",
    "flat_precision_df.to_csv('C:/kv/Thesus/PLAStiCC/results/smote/6deep_flat_precision.csv')\n",
    "flat_recall_df.to_csv('C:/kv/Thesus/PLAStiCC/results/smote/6deep_flat_recall1.csv')\n",
    "flat_f1_df.to_csv('C:/kv/Thesus/PLAStiCC/results/smote/6deep_flat_f1_1.csv')\n",
    "\n",
    "stack_precision_df.to_csv('C:/kv/Thesus/PLAStiCC/results/smote/6deep_stack_precision.csv')\n",
    "stack_recall_df.to_csv('C:/kv/Thesus/PLAStiCC/results/smote/6deep_stack_recall.csv')\n",
    "stack_f1_df.to_csv('C:/kv/Thesus/PLAStiCC/results/smote/6deep_stack_f1.csv')\n",
    "\n",
    "\n",
    "ancestor_precision_df.to_csv('C:/kv/Thesus/PLAStiCC/results/smote/6deep_ancestor_precision.csv')\n",
    "ancestor_recall_df.to_csv('C:/kv/Thesus/PLAStiCC/results/smote/6deep_ancestor_recall.csv')\n",
    "ancestor_f1_df.to_csv('C:/kv/Thesus/PLAStiCC/results/smote/6deep_ancestor_f1.csv')\n",
    "\n",
    "\n",
    "parent_precision_df.to_csv('C:/kv/Thesus/PLAStiCC/results/smote/6deep_parent_precision.csv')\n",
    "parent_recall_df.to_csv('C:/kv/Thesus/PLAStiCC/results/smote/6deep_parent_recall.csv')\n",
    "parent_f1_df.to_csv('C:/kv/Thesus/PLAStiCC/results/smote/6deep_parent_f1.csv')\n",
    "\n",
    "\n",
    "target_precision_df.to_csv('C:/kv/Thesus/PLAStiCC/results/smote/6deep_target_precision.csv')\n",
    "target_recall_df.to_csv('C:/kv/Thesus/PLAStiCC/results/smote/6deep_target_recall.csv')\n",
    "target_f1_df.to_csv('C:/kv/Thesus/PLAStiCC/results/smote/6deep_target_f1.csv')\n",
    "\n",
    "flat_acc_df.to_csv('C:/kv/Thesus/PLAStiCC/results/smote/6deep_flat_acc.csv')\n",
    "stack_acc_df.to_csv('C:/kv/Thesus/PLAStiCC/results/smote/6deep_stack_acc.csv')\n",
    "ancestor_acc_df.to_csv('C:/kv/Thesus/PLAStiCC/results/smote/6deep_ancestor_acc.csv')\n",
    "parent_acc_df.to_csv('C:/kv/Thesus/PLAStiCC/results/smote/6deep_parent_acc.csv')\n",
    "target_acc_df.to_csv('C:/kv/Thesus/PLAStiCC/results/smote/6deep_target_acc.csv')\n",
    "\n",
    "\n",
    "\n",
    "del /\n",
    "stack_precision_df,stack_recall_df,stack_f1_df,/\n",
    "flat_precision_df,flat_recall_df,flat_f1_df,/\n",
    "ancestor_precision_df,ancestor_recall_df,ancestor_f1_df,/\n",
    "parent_precision_df,parent_recall_df,parent_f1_df,/\n",
    "target_precision_df,target_recall_df,target_f1_df,/\n",
    "flat_acc_df,stack_acc_df,ancestor_acc_df,parent_acc_df,target_acc_df/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score,make_scorer,f1_score\n",
    "\n",
    "model = KerasClassifier(build_fn=flat_model,verbose=0)\n",
    "batch_size = [100, 500, 1000, 2000]\n",
    "epochs = [100,200,300]\n",
    "#optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "#param_grid = dict(optimizer=optimizer)\n",
    "#learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "#activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "#units = [8,10,12,14,16]\n",
    "#param_grid = dict(units=units)\n",
    "#weight_constraint = [1, 2, 3, 4, 5]\n",
    "#dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "param_grid = dict(batch_size=batch_size,epochs=epochs)\n",
    "#scoring = {'bal_accuracy': make_scorer(balanced_accuracy_score)}\n",
    "#f1_scorer = make_scorer(f1_score,average='micro',greater_is_better=True)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(flat_X_train,flat_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Best: 0.701065 using {'optimizer': 'Adam'}\n",
    "#0.666058 (0.011317) with: {'optimizer': 'SGD'}\n",
    "#0.688280 (0.012305) with: {'optimizer': 'RMSprop'}\n",
    "#0.674886 (0.008202) with: {'optimizer': 'Adagrad'}\n",
    "#0.692542 (0.007165) with: {'optimizer': 'Adadelta'}\n",
    "#0.701065 (0.007761) with: {'optimizer': 'Adam'}\n",
    "#0.686454 (0.008642) with: {'optimizer': 'Adamax'}\n",
    "#0.687671 (0.008066) with: {'optimizer': 'Nadam'}\n",
    "\n",
    "\n",
    "#Best: 0.690107 using {'learn_rate': 0.001}\n",
    "#0.690107 (0.006766) with: {'learn_rate': 0.001}\n",
    "#0.656621 (0.006501) with: {'learn_rate': 0.01}\n",
    "#0.603957 (0.028003) with: {'learn_rate': 0.1}\n",
    "#0.542770 (0.012705) with: {'learn_rate': 0.2}\n",
    "#0.512329 (0.032133) with: {'learn_rate': 0.3}\n",
    "\n",
    "\n",
    "\n",
    "#Best: 0.704110 using {'init_mode': 'normal'}\n",
    "#0.693455 (0.010607) with: {'init_mode': 'uniform'}\n",
    "#0.687976 (0.006027) with: {'init_mode': 'lecun_uniform'}\n",
    "#0.704110 (0.016856) with: {'init_mode': 'normal'}\n",
    "#0.529680 (0.009041) with: {'init_mode': 'zero'}\n",
    "#0.697717 (0.008469) with: {'init_mode': 'glorot_normal'}\n",
    "#0.695586 (0.008214) with: {'init_mode': 'glorot_uniform'}\n",
    "#0.693151 (0.012455) with: {'init_mode': 'he_normal'}\n",
    "#0.692542 (0.006073) with: {'init_mode': 'he_uniform'}\n",
    "\n",
    "#Best: 0.702892 using {'activation': 'softplus'}\n",
    "#0.654795 (0.009163) with: {'activation': 'softmax'}\n",
    "#0.702892 (0.003444) with: {'activation': 'softplus'}\n",
    "#0.698630 (0.006834) with: {'activation': 'softsign'}\n",
    "#0.689193 (0.018300) with: {'activation': 'relu'}\n",
    "#0.695282 (0.008642) with: {'activation': 'tanh'}\n",
    "#0.691020 (0.004965) with: {'activation': 'sigmoid'}\n",
    "#0.683714 (0.005496) with: {'activation': 'hard_sigmoid'}\n",
    "#0.650228 (0.001292) with: {'activation': 'linear'}\n",
    "\n",
    "\n",
    "#Best: 0.708371 using {'units': 12}\n",
    "#0.699239 (0.009442) with: {'units': 8}\n",
    "#0.698630 (0.002237) with: {'units': 10}\n",
    "#0.708371 (0.007616) with: {'units': 12}\n",
    "#0.706240 (0.006766) with: {'units': 14}\n",
    "#0.705632 (0.005792) with: {'units': 16}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
