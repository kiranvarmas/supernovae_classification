{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kiran\\Anaconda3\\envs\\ML\\lib\\site-packages\\numpy\\core\\__init__.py:29: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\kiran\\Anaconda3\\envs\\ML\\lib\\site-packages\\numpy\\.libs\\libopenblas.CSRRD7HKRKC3T3YXA7VY7TAZGLSWDKW6.gfortran-win_amd64.dll\n",
      "C:\\Users\\kiran\\Anaconda3\\envs\\ML\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "rn.seed(10)\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(10)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_features_data = pd.read_csv(\"C:/kv/Thesus/Docs/filtered_features_data.csv\",header=0)\n",
    "\n",
    "\n",
    "print(np.unique(filtered_features_data.sub_label,return_counts=True))\n",
    "print(np.unique(filtered_features_data.main_label,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = filtered_features_data[filtered_features_data.columns.difference(['source_id','sub_label','main_label'])]\n",
    "#y_main = filtered_features_data.main_label\n",
    "#y_sub = filtered_features_data.sub_label\n",
    "y = filtered_features_data[['sub_label','main_label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ceil(X.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n",
    "scaler = RobustScaler()\n",
    "#scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "X_scl = scaler.fit_transform(X) #X_train #X_train_res\n",
    "\n",
    "X_scl = pd.DataFrame(X_scl,columns=X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ceil(X_scl.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBoxPlot(dataset):\n",
    "    plot=dataset.boxplot(by='sub_label',showmeans=True)\n",
    "    fig = plot.get_figure()\n",
    "    fig.suptitle('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getBoxPlot(filtered_features_data[['Amplitude','sub_label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "rn.seed(10)\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(10)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "flat_recall_results = []\n",
    "flat_precision_results=[]\n",
    "flat_accuracy_results=[]\n",
    "flat_balanced_accuracy=[]\n",
    "flat_f1_val=[]\n",
    "\n",
    "\n",
    "stack_recall_results = []\n",
    "stack_precision_results=[]\n",
    "stack_accuracy_results=[]\n",
    "stack_balanced_accuracy=[]\n",
    "stack_f1_val=[]\n",
    "\n",
    "\n",
    "ancestor_recall_results = []\n",
    "ancestor_precision_results=[]\n",
    "ancestor_accuracy_results=[]\n",
    "ancestor_balanced_accuracy=[]\n",
    "ancestor_f1_val=[]\n",
    "\n",
    "\n",
    "parent_recall_results = []\n",
    "parent_precision_results=[]\n",
    "parent_accuracy_results=[]\n",
    "parent_balanced_accuracy=[]\n",
    "parent_f1_val=[]\n",
    "\n",
    "target_recall_results = []\n",
    "target_precision_results=[]\n",
    "target_accuracy_results=[]\n",
    "target_balanced_accuracy=[]\n",
    "target_f1_val=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plastic_data = pd.read_csv('C:/kv/Thesus/PLAStiCC/plasticc_filtered_data.csv',header=0)\n",
    "plastic_data = pd.read_csv('C:/kv/Thesus/PLAStiCC/plasticc_non_filtered_data.csv',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_pla_data.columns[full_pla_data.isnull().any()]\n",
    "plastic_data.columns[plastic_data.isnull().any()]\n",
    "plastic_data = plastic_data.fillna(value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plastic_data.info()\n",
    "#plastic_data = plastic_data.fillna(value=0)\n",
    "\n",
    "\n",
    "#plastic_data = full_pla_data.fillna(value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4381, 197)\n",
      "(4381, 3)\n",
      "(array([42, 52, 62, 67, 90], dtype=int64), array([1193,  183,  484,  208, 2313], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "X = plastic_data[plastic_data.columns.difference(['object_id','target','parent_label','ancestor_label'])]\n",
    "y = plastic_data[['target','parent_label','ancestor_label']]\n",
    "\n",
    "X=X.astype(float)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(np.unique(y.target,return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not required now\n",
    "def flat_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=197, activation='softplus',kernel_initializer=glorot_normal(seed=100)))\n",
    "    model.add(Dense(102,activation='softplus'))\n",
    "\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def stack_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(205, input_dim=202, activation='softplus',kernel_initializer=glorot_normal(seed=100)))\n",
    "    model.add(Dense(105,activation='softplus'))\n",
    "\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def ancestor_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=197, activation='softplus',kernel_initializer=glorot_normal(seed=100)))\n",
    "   \n",
    "    model.add(Dense(101,activation='softplus')) #12 10 8 \n",
    "\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def parent_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=199, activation='softplus',kernel_initializer=glorot_normal(seed=100)))\n",
    "\n",
    "    model.add(Dense(102,activation='softplus')) #12 60 bal\n",
    "\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    optimizer = Adagrad(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def target_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(205, input_dim=202, activation='softplus',kernel_initializer=glorot_normal(seed=100)))\n",
    "    model.add(Dense(105,activation='softplus'))\n",
    "\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "from keras.layers import Input,Dense,Dropout\n",
    "from keras.models import Model,Sequential\n",
    "from keras.initializers import glorot_normal,glorot_uniform\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.optimizers import Adam,Adagrad\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "def deep_flat_model():\n",
    "    model = Sequential() #200 150 100 75 25 5\n",
    "    model.add(Dense(200, input_dim=197, activation='softplus',kernel_initializer=glorot_normal(seed=100)))\n",
    "    model.add(Dense(175,activation='softplus'))\n",
    "    model.add(Dense(150,activation='softplus'))\n",
    "    model.add(Dense(125,activation='softplus'))\n",
    "    model.add(Dense(100,activation='softplus'))\n",
    "    model.add(Dense(75,activation='softplus'))\n",
    "    model.add(Dense(50,activation='softplus'))\n",
    "    model.add(Dense(25,activation='softplus'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def deep_stack_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(205, input_dim=202, activation='softplus',kernel_initializer=glorot_normal(seed=100)))\n",
    "    model.add(Dense(175,activation='softplus'))\n",
    "    model.add(Dense(150,activation='softplus'))\n",
    "    model.add(Dense(125,activation='softplus'))\n",
    "    model.add(Dense(100,activation='softplus'))\n",
    "    model.add(Dense(75,activation='softplus'))\n",
    "    model.add(Dense(50,activation='softplus'))\n",
    "    model.add(Dense(25,activation='softplus'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def deep_ancestor_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=197, activation='softplus',kernel_initializer=glorot_normal(seed=100)))\n",
    "    model.add(Dense(175,activation='softplus'))\n",
    "    model.add(Dense(150,activation='softplus'))\n",
    "    model.add(Dense(125,activation='softplus'))\n",
    "    model.add(Dense(100,activation='softplus'))\n",
    "    model.add(Dense(75,activation='softplus'))\n",
    "    model.add(Dense(50,activation='softplus'))\n",
    "    model.add(Dense(25,activation='softplus'))\n",
    "\n",
    "\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def deep_parent_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=199, activation='softplus',kernel_initializer=glorot_normal(seed=100)))\n",
    "    \n",
    "    model.add(Dense(175,activation='softplus'))\n",
    "    model.add(Dense(150,activation='softplus'))\n",
    "    model.add(Dense(125,activation='softplus'))\n",
    "    model.add(Dense(100,activation='softplus'))\n",
    "    model.add(Dense(75,activation='softplus'))\n",
    "    model.add(Dense(50,activation='softplus'))\n",
    "    model.add(Dense(25,activation='softplus'))\n",
    "\n",
    "\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    optimizer = Adagrad(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def deep_target_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(205, input_dim=202, activation='softplus',kernel_initializer=glorot_normal(seed=100)))\n",
    "    model.add(Dense(175,activation='softplus'))\n",
    "    model.add(Dense(150,activation='softplus'))\n",
    "    model.add(Dense(125,activation='softplus'))\n",
    "    model.add(Dense(100,activation='softplus'))\n",
    "    model.add(Dense(75,activation='softplus'))\n",
    "    model.add(Dense(50,activation='softplus'))\n",
    "    model.add(Dense(25,activation='softplus'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _flat_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_dim=20, activation='softplus',kernel_initializer=glorot_normal(seed=10)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(15,activation='softplus'))\n",
    "\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def _ancestor_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_dim=20, activation='softplus',kernel_initializer=glorot_normal(seed=10)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(13,activation='softplus')) #12 10 8 \n",
    "\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def _parent_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_dim=22, activation='softplus',kernel_initializer=glorot_normal(seed=10)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(14,activation='softplus')) #12 60 bal\n",
    "\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    optimizer = Adagrad(lr=0.01)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def _target_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_dim=25, activation='softplus',kernel_initializer=glorot_normal(seed=10)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(15,activation='softplus'))\n",
    "\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "\n",
    "def getTrainTestData(X,y,random_seed):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=random_seed)\n",
    "    X_train=X_train.reset_index(drop=True)\n",
    "    X_test =X_test.reset_index(drop=True)\n",
    "    y_train=y_train.reset_index(drop=True)\n",
    "    y_test =y_test.reset_index(drop=True)\n",
    "    \n",
    "    return X_train,X_test,y_train,y_test\n",
    "\n",
    "def performScaling(X_train,X_test,y_train,y_test,col_list):\n",
    "\n",
    "    from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n",
    "    scaler = RobustScaler()\n",
    "    #scaler = MinMaxScaler()\n",
    "    X_train_scl = scaler.fit_transform(X_train) #X_train #X_train_res\n",
    "    X_test_scl = scaler.transform(X_test)\n",
    "\n",
    "    X_train_scl = pd.DataFrame(X_train_scl, columns = col_list) #X-train_scl\n",
    "    X_test_scl  = pd.DataFrame(X_test_scl, columns = col_list)\n",
    "\n",
    "    y_train=y_train.reset_index(drop=True)\n",
    "    y_test =y_test.reset_index(drop=True)\n",
    "\n",
    "    scaled_train_df = pd.concat([X_train_scl,y_train],axis=1)\n",
    "    scaled_test_df = pd.concat([X_test_scl,y_test],axis=1)\n",
    "    \n",
    "    return scaled_train_df,scaled_test_df\n",
    "\n",
    "\n",
    "def get_estimator(model,X,y,weights):\n",
    "    estimator = train_model(model,X,y)\n",
    "    estimator.fit(X,y,class_weight=weights)\n",
    " \n",
    "    return estimator\n",
    "\n",
    "def train_model(base,X,y):\n",
    "    \n",
    "    from sklearn.model_selection import StratifiedKFold,cross_val_score  \n",
    "    estimator = KerasClassifier(build_fn=base, epochs=200, batch_size=100,verbose=0)\n",
    "   \n",
    "        \n",
    "    #kfold = StratifiedKFold(n_splits=10, shuffle=True,random_state=100)#random_state=0\n",
    "    #results = cross_val_score(estimator,X,y, cv=kfold,scoring='balanced_accuracy')                              \n",
    "    #print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))  \n",
    "    print(\"skipped cv\")\n",
    "    return estimator\n",
    "\n",
    "def evaluate_model(classifier,X,y):\n",
    "   \n",
    "    \n",
    "    predictions,predict_prob,accuracy,bal_accuracy,precision,recall,f1,confusion_mat = get_metrics(classifier,X,y)\n",
    "    \n",
    "    print_metrics(accuracy,bal_accuracy,precision,recall,f1,confusion_mat)\n",
    "    \n",
    "    return predictions,predict_prob,accuracy,bal_accuracy,precision,recall,f1,confusion_mat\n",
    "\n",
    "def get_metrics(classifier,X,y):\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,balanced_accuracy_score,f1_score\n",
    "    predictions,predict_prob = get_predictions(classifier,X)\n",
    "    accuracy = accuracy_score(y,predictions)\n",
    "    balanced_accuracy= balanced_accuracy_score(y,predictions)\n",
    "    precision=precision_score(y,predictions,average=None)\n",
    "    recall = recall_score(y,predictions,average=None)\n",
    "    confusion_mat = confusion_matrix(y,predictions)\n",
    "    f1 = f1_score(y, predictions, labels=None, pos_label=1,average=None, sample_weight=None)\n",
    "    \n",
    "    return predictions,predict_prob,accuracy,balanced_accuracy,precision,recall,f1,confusion_mat\n",
    "\n",
    "def get_predictions(classifier,X):\n",
    "    return classifier.predict(X), classifier.predict_proba(X)\n",
    "\n",
    "\n",
    "def print_metrics(accuracy,balanced_accuracy,precision,recall,f1,confusion_mat):    \n",
    "    print(\" Accuracy\", accuracy)\n",
    "    print(\" Balanced accuracy\",balanced_accuracy)\n",
    "    print(\" Recall\",recall )\n",
    "    print(\" Precision\",precision)\n",
    "    print(\"f1\",f1)\n",
    "    print(confusion_mat)\n",
    "    \n",
    "    \n",
    "def getExperimentData(train_df,test_df,label_type):\n",
    "    \n",
    "    X_train = train_df[train_df.columns.difference(['ancestor_label','parent_label','target'])]\n",
    "    X_test = test_df[test_df.columns.difference(['ancestor_label','parent_label','target'])]\n",
    "    \n",
    "    if (label_type==\"ancestor\"):\n",
    "        \n",
    "        Y_train = train_df[['ancestor_label']]\n",
    "        Y_test = test_df[['ancestor_label']]\n",
    "    \n",
    "    elif(label_type==\"parent\"):\n",
    "       \n",
    "        Y_train = train_df[['parent_label']]\n",
    "        Y_test = test_df[['parent_label']]\n",
    "    \n",
    "    else:\n",
    "        Y_train = train_df[['target']]\n",
    "        Y_test = test_df[['target']]\n",
    "    \n",
    "    \n",
    "    print(label_type + \"X_train:\", X_train.shape)\n",
    "    print(label_type + \"Y_train:\", Y_train.shape)\n",
    "\n",
    "    print(label_type + \"X_test\", X_test.shape)\n",
    "    print(label_type + \"Y_test\", Y_test.shape)\n",
    "\n",
    "    print(label_type + \"Y_train:\", np.unique(Y_train,return_counts=True))\n",
    "    print(label_type + \"Y_test:\", np.unique( Y_test,return_counts=True))\n",
    "    print(\"\\n \\n\")\n",
    "    \n",
    "    return X_train,X_test,Y_train,Y_test\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_probs(dataset,predict_prob,method):\n",
    "    if method==\"stack\":\n",
    "        ip = pd.DataFrame(predict_prob, columns=['prob1','prob2','prob3','prob4','prob5'])\n",
    "        X = dataset.copy()\n",
    "        X['prob1']=ip[['prob1']]\n",
    "        X['prob2']=ip[['prob2']]\n",
    "        X['prob3']=ip[['prob3']]\n",
    "        X['prob4']=ip[['prob4']]\n",
    "        X['prob5']=ip[['prob5']]\n",
    "        \n",
    "    elif method==\"ancestor\":\n",
    "        ip = pd.DataFrame(predict_prob, columns=['prob1','prob2'])\n",
    "        X = dataset.copy()\n",
    "        X['prob1']=ip[['prob1']]\n",
    "        X['prob2']=ip[['prob2']]\n",
    "    \n",
    "    elif method==\"parent\":\n",
    "        ip = pd.DataFrame(predict_prob, columns=['prob3','prob4','prob5'])\n",
    "        X = dataset.copy()\n",
    "        X['prob3']=ip[['prob3']]\n",
    "        X['prob4']=ip[['prob4']]\n",
    "        X['prob5']=ip[['prob5']]\n",
    "\n",
    "        \n",
    "    return X\n",
    "\n",
    "def put_in_df(metric,hierarchy):\n",
    "    \n",
    "    if (hierarchy==\"accuracy\"):\n",
    "        headers = ['accuracy','bal_accuracy']\n",
    "    elif (hierarchy==\"ancestor\"):\n",
    "        headers = ['class42','class100']\n",
    "    elif (hierarchy==\"parent\"):\n",
    "        \n",
    "        headers = ['class42','class62','class90']\n",
    "        \n",
    "    else:\n",
    "        headers= ['class42','class52','class62','class67','class90']\n",
    "    \n",
    "    df = pd.DataFrame(metric,columns=headers)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del parent_X_train,parent_X_test,parent_Y_train,parent_Y_test\n",
    "#del target_X_train,target_X_test,target_Y_train,target_Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatX_train: (3285, 197)\n",
      "flatY_train: (3285, 1)\n",
      "flatX_test (1096, 197)\n",
      "flatY_test (1096, 1)\n",
      "flatY_train: (array([42, 52, 62, 67, 90], dtype=int64), array([ 898,  141,  360,  140, 1746], dtype=int64))\n",
      "flatY_test: (array([42, 52, 62, 67, 90], dtype=int64), array([295,  42, 124,  68, 567], dtype=int64))\n",
      "\n",
      " \n",
      "\n",
      "ancestorX_train: (3285, 197)\n",
      "ancestorY_train: (3285, 1)\n",
      "ancestorX_test (1096, 197)\n",
      "ancestorY_test (1096, 1)\n",
      "ancestorY_train: (array([ 42, 100], dtype=int64), array([ 898, 2387], dtype=int64))\n",
      "ancestorY_test: (array([ 42, 100], dtype=int64), array([295, 801], dtype=int64))\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "col = X.columns\n",
    "X_train,X_test,y_train,y_test = getTrainTestData(X,y,100)\n",
    "\n",
    "scaled_train_df, scaled_test_df = performScaling(X_train,X_test,y_train,y_test,col)\n",
    "\n",
    "flat_X_train,flat_X_test,flat_Y_train,flat_Y_test = getExperimentData(scaled_train_df, scaled_test_df,\"flat\")\n",
    "ancestor_X_train,ancestor_X_test,ancestor_Y_train,ancestor_Y_test = getExperimentData(scaled_train_df, scaled_test_df,\"ancestor\")\n",
    "#parent_X_train,parent_X_test,parent_Y_train,parent_Y_test = getExperimentData(scaled_train_df, scaled_test_df,\"parent\")\n",
    "#target_X_train,target_X_test,target_Y_train,target_Y_test = getExperimentData(scaled_train_df, scaled_test_df,\"target\")\n",
    "\n",
    "#del scaled_train_df, scaled_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped cv\n",
      " Accuracy 0.7244525547445255\n",
      " Balanced accuracy 0.5890094579435161\n",
      " Recall [0.61016949 0.30952381 0.55645161 0.60294118 0.8659612 ]\n",
      " Precision [0.6870229  0.40625    0.60526316 0.68333333 0.78184713]\n",
      "f1 [0.64631957 0.35135135 0.57983193 0.640625   0.82175732]\n",
      "[[180   3  23   5  84]\n",
      " [  8  13   9   0  12]\n",
      " [ 17   2  69   7  29]\n",
      " [  6   2   7  41  12]\n",
      " [ 51  12   6   7 491]]\n"
     ]
    }
   ],
   "source": [
    "weights={}\n",
    "\n",
    "flat_estimator = get_estimator(deep_flat_model,flat_X_train,flat_Y_train,weights)\n",
    "#base_estimator = get_estimator(base_class_model,base_X_train,base_Y_train,50,weights)\n",
    "\n",
    "flat_train_predictions,flat_train_predict_prob = get_predictions(flat_estimator,flat_X_train)\n",
    "\n",
    "flat_predictions,flat_predict_prob,flat_accuracy,flat_bal_accuracy,flat_precision,flat_recall,flat_f1,flat_confusion_mat=evaluate_model(flat_estimator,flat_X_test,flat_Y_test)\n",
    "\n",
    "flat_recall_results.append(flat_recall)\n",
    "flat_precision_results.append(flat_precision)\n",
    "flat_accuracy_results.append([flat_accuracy,flat_bal_accuracy])\n",
    "flat_f1_val.append(flat_f1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7235401459854015, 0.5362469933400869],\n",
       " [0.7271897810218978, 0.5864335837643185],\n",
       " [0.7144160583941606, 0.5508466026080988],\n",
       " [0.7299270072992701, 0.563215474991772],\n",
       " [0.7208029197080292, 0.5354023004226104],\n",
       " [0.7052919708029197, 0.5331797186179359],\n",
       " [0.708941605839416, 0.5343011805366876],\n",
       " [0.6806569343065694, 0.5510141996527025],\n",
       " [0.7235401459854015, 0.5631465600142679],\n",
       " [0.7244525547445255, 0.5890094579435161]]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3285, 202)\n",
      "(1096, 202)\n",
      "(array([42, 52, 62, 67, 90], dtype=int64), array([ 898,  141,  360,  140, 1746], dtype=int64))\n",
      "(array([42, 52, 62, 67, 90], dtype=int64), array([295,  42, 124,  68, 567], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "stack_X_train = get_probs(flat_X_train,flat_train_predict_prob,\"stack\")\n",
    "stack_X_test = get_probs(flat_X_test,flat_predict_prob,\"stack\")\n",
    "\n",
    "\n",
    "stack_Y_train= scaled_train_df[['target']]\n",
    "stack_Y_test = scaled_test_df[['target']]\n",
    "\n",
    "\n",
    "print(stack_X_train.shape)\n",
    "print(stack_X_test.shape)\n",
    "print(np.unique(stack_Y_train,return_counts=True))\n",
    "\n",
    "print(np.unique(stack_Y_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped cv\n",
      " Accuracy 0.7262773722627737\n",
      " Balanced accuracy 0.5969242151227379\n",
      " Recall [0.72542373 0.30952381 0.5483871  0.58823529 0.81305115]\n",
      " Precision [0.60623229 0.36111111 0.60714286 0.70175439 0.85687732]\n",
      "f1 [0.66049383 0.33333333 0.57627119 0.64       0.83438914]\n",
      "[[214   4  23   5  49]\n",
      " [ 10  13   8   0  11]\n",
      " [ 36   3  68   7  10]\n",
      " [ 10   4   7  40   7]\n",
      " [ 83  12   6   5 461]]\n"
     ]
    }
   ],
   "source": [
    "weights={}\n",
    "\n",
    "stack_estimator = get_estimator(deep_stack_model,stack_X_train,stack_Y_train,weights)\n",
    "#base_estimator = get_estimator(base_class_model,base_X_train,base_Y_train,50,weights)\n",
    "\n",
    "stack_train_predictions,stack_train_predict_prob = get_predictions(stack_estimator,stack_X_train)\n",
    "\n",
    "stack_predictions,stack_predict_prob,stack_accuracy,stack_bal_accuracy,stack_precision,stack_recall,stack_f1,stack_confusion_mat=evaluate_model(stack_estimator,stack_X_test,stack_Y_test)\n",
    "\n",
    "stack_recall_results.append(stack_recall)\n",
    "stack_precision_results.append(stack_precision)\n",
    "stack_accuracy_results.append([stack_accuracy,stack_bal_accuracy])\n",
    "stack_f1_val.append(stack_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7235401459854015, 0.5294421094231463],\n",
       " [0.7226277372262774, 0.5808526173231643],\n",
       " [0.7052919708029197, 0.5339351407000686],\n",
       " [0.7226277372262774, 0.5553448695853362],\n",
       " [0.7153284671532847, 0.541241923414332],\n",
       " [0.698905109489051, 0.5462321930206662],\n",
       " [0.7107664233576643, 0.5423266230145638],\n",
       " [0.698905109489051, 0.5263465613098772],\n",
       " [0.7262773722627737, 0.5463836210554318],\n",
       " [0.7262773722627737, 0.5969242151227379]]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stack_accuracy_results.pop()\n",
    "#stack_precision_results.pop()\n",
    "#stack_recall_results.pop()\n",
    "#stack_f1_val.pop()\n",
    "stack_accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped cv\n",
      " Accuracy 0.8166058394160584\n",
      " Balanced accuracy 0.7406948940942466\n",
      " Recall [0.57627119 0.9051186 ]\n",
      " Precision [0.69105691 0.85294118]\n",
      "f1 [0.6284658 0.8782556]\n",
      "[[170 125]\n",
      " [ 76 725]]\n"
     ]
    }
   ],
   "source": [
    "weights={}\n",
    "\n",
    "ancestor_estimator = get_estimator(deep_ancestor_model,ancestor_X_train,ancestor_Y_train,weights)\n",
    "#base_estimator = get_estimator(base_class_model,base_X_train,base_Y_train,50,weights)\n",
    "\n",
    "ancestor_train_predictions,ancestor_train_predict_prob = get_predictions(ancestor_estimator,ancestor_X_train)\n",
    "\n",
    "ancestor_predictions,ancestor_predict_prob,ancestor_accuracy,ancestor_bal_accuracy,ancestor_precision,ancestor_recall,ancestor_f1,ancestor_confusion_mat=evaluate_model(ancestor_estimator,ancestor_X_test,ancestor_Y_test)\n",
    "\n",
    "ancestor_recall_results.append(ancestor_recall)\n",
    "ancestor_precision_results.append(ancestor_precision)\n",
    "ancestor_accuracy_results.append([ancestor_accuracy,ancestor_bal_accuracy])\n",
    "ancestor_f1_val.append(ancestor_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.802007299270073, 0.7810860083145914],\n",
       " [0.823905109489051, 0.7385575177702004],\n",
       " [0.7956204379562044, 0.7275958302552737],\n",
       " [0.7928832116788321, 0.7305850123718629],\n",
       " [0.8047445255474452, 0.7336838288614937],\n",
       " [0.8138686131386861, 0.6995651970639114],\n",
       " [0.7974452554744526, 0.7155605670103093],\n",
       " [0.7846715328467153, 0.750088193166411],\n",
       " [0.7947080291970803, 0.6981398544455188],\n",
       " [0.8166058394160584, 0.7406948940942466]]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ancestor_accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3285, 199)\n",
      "(1096, 199)\n",
      "(array([42, 62, 90], dtype=int64), array([ 898,  360, 2027], dtype=int64))\n",
      "(array([42, 62, 90], dtype=int64), array([295, 124, 677], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "parent_X_train = get_probs(ancestor_X_train,ancestor_train_predict_prob,\"ancestor\")\n",
    "parent_X_test = get_probs(ancestor_X_test,ancestor_predict_prob,\"ancestor\")\n",
    "\n",
    "\n",
    "parent_Y_train= scaled_train_df[['parent_label']]\n",
    "parent_Y_test = scaled_test_df[['parent_label']]\n",
    "\n",
    "\n",
    "print(parent_X_train.shape)\n",
    "print(parent_X_test.shape)\n",
    "print(np.unique(parent_Y_train,return_counts=True))\n",
    "\n",
    "print(np.unique(parent_Y_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped cv\n",
      " Accuracy 0.7518248175182481\n",
      " Balanced accuracy 0.6355774586312377\n",
      " Recall [0.60338983 0.42741935 0.87592319]\n",
      " Precision [0.70355731 0.50961538 0.80243572]\n",
      "f1 [0.64963504 0.46491228 0.83757062]\n",
      "[[178  21  96]\n",
      " [ 21  53  50]\n",
      " [ 54  30 593]]\n"
     ]
    }
   ],
   "source": [
    "weights={}\n",
    "\n",
    "parent_estimator = get_estimator(deep_parent_model,parent_X_train,parent_Y_train,weights)\n",
    "#base_estimator = get_estimator(base_class_model,base_X_train,base_Y_train,50,weights)\n",
    "\n",
    "parent_train_predictions,parent_train_predict_prob = get_predictions(parent_estimator,parent_X_train)\n",
    "\n",
    "parent_predictions,parent_predict_prob,parent_accuracy,parent_bal_accuracy,parent_precision,parent_recall,parent_f1,parent_confusion_mat=evaluate_model(parent_estimator,parent_X_test,parent_Y_test)\n",
    "\n",
    "parent_recall_results.append(parent_recall)\n",
    "parent_precision_results.append(parent_precision)\n",
    "parent_accuracy_results.append([parent_accuracy,parent_bal_accuracy])\n",
    "parent_f1_val.append(parent_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7655109489051095, 0.6655099159709276],\n",
       " [0.7408759124087592, 0.6591015239202872],\n",
       " [0.7317518248175182, 0.6414546990221875],\n",
       " [0.7290145985401459, 0.6058000658333148],\n",
       " [0.7536496350364964, 0.6497131429846722],\n",
       " [0.7427007299270073, 0.6114349218387899],\n",
       " [0.7618613138686131, 0.6423009500960154],\n",
       " [0.7390510948905109, 0.6472206256831674],\n",
       " [0.7518248175182481, 0.6355774586312377]]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3285, 202)\n",
      "(1096, 202)\n",
      "(array([42, 52, 62, 67, 90], dtype=int64), array([ 898,  141,  360,  140, 1746], dtype=int64))\n",
      "(array([42, 52, 62, 67, 90], dtype=int64), array([295,  42, 124,  68, 567], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "target_X_train = get_probs(parent_X_train,parent_train_predict_prob,\"parent\")\n",
    "target_X_test = get_probs(parent_X_test,parent_predict_prob,\"parent\")\n",
    "\n",
    "target_Y_train= scaled_train_df[['target']]\n",
    "target_Y_test = scaled_test_df[['target']]\n",
    "\n",
    "print(target_X_train.shape)\n",
    "print(target_X_test.shape)\n",
    "print(np.unique(target_Y_train,return_counts=True))\n",
    "print(np.unique(target_Y_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped cv\n",
      " Accuracy 0.708029197080292\n",
      " Balanced accuracy 0.5678855684184858\n",
      " Recall [0.69830508 0.35714286 0.48387097 0.48529412 0.81481481]\n",
      " Precision [0.59195402 0.45454545 0.55045872 0.55       0.84615385]\n",
      "f1 [0.6407465  0.4        0.51502146 0.515625   0.83018868]\n",
      "[[206   5  23   6  55]\n",
      " [  9  15   4   3  11]\n",
      " [ 40   4  60  10  10]\n",
      " [ 11   1  15  33   8]\n",
      " [ 82   8   7   8 462]]\n"
     ]
    }
   ],
   "source": [
    "weights={}\n",
    "\n",
    "target_estimator = get_estimator(deep_target_model,target_X_train,target_Y_train,weights)\n",
    "#base_estimator = get_estimator(base_class_model,base_X_train,base_Y_train,50,weights)\n",
    "\n",
    "target_train_predictions,target_train_predict_prob = get_predictions(target_estimator,target_X_train)\n",
    "\n",
    "target_predictions,target_predict_prob,target_accuracy,target_bal_accuracy,target_precision,target_recall,target_f1,target_confusion_mat=evaluate_model(target_estimator,target_X_test,target_Y_test)\n",
    "\n",
    "target_recall_results.append(target_recall)\n",
    "target_precision_results.append(target_precision)\n",
    "target_accuracy_results.append([target_accuracy,target_bal_accuracy])\n",
    "target_f1_val.append(target_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7235401459854015, 0.5362469933400869],\n",
       " [0.7271897810218978, 0.5864335837643185],\n",
       " [0.7144160583941606, 0.5508466026080988],\n",
       " [0.7299270072992701, 0.563215474991772],\n",
       " [0.7208029197080292, 0.5354023004226104],\n",
       " [0.7052919708029197, 0.5331797186179359],\n",
       " [0.708941605839416, 0.5343011805366876],\n",
       " [0.6806569343065694, 0.5510141996527025],\n",
       " [0.7235401459854015, 0.5631465600142679],\n",
       " [0.7244525547445255, 0.5890094579435161]]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_accuracy_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7235401459854015, 0.5294421094231463],\n",
       " [0.7226277372262774, 0.5808526173231643],\n",
       " [0.7052919708029197, 0.5339351407000686],\n",
       " [0.7226277372262774, 0.5553448695853362],\n",
       " [0.7153284671532847, 0.541241923414332],\n",
       " [0.698905109489051, 0.5462321930206662],\n",
       " [0.7107664233576643, 0.5423266230145638],\n",
       " [0.698905109489051, 0.5263465613098772],\n",
       " [0.7262773722627737, 0.5463836210554318],\n",
       " [0.7262773722627737, 0.5969242151227379]]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7135036496350365, 0.5107992320430241],\n",
       " [0.7135036496350365, 0.5548645358215778],\n",
       " [0.6916058394160584, 0.5209224433768016],\n",
       " [0.6852189781021898, 0.5217055330738709],\n",
       " [0.6961678832116789, 0.5307008662567562],\n",
       " [0.6833941605839416, 0.5671132406539149],\n",
       " [0.697992700729927, 0.5289328908416683],\n",
       " [0.6934306569343066, 0.5270292741597402],\n",
       " [0.6888686131386861, 0.5241188659259465],\n",
       " [0.708029197080292, 0.5678855684184858]]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_precision_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_precision_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_precision_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_recall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_recall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_recall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    class42   class52   class62   class67   class90\n",
       " 0  0.709163  0.243243  0.626263  0.477273  0.769925\n",
       " 1  0.695312  0.236842  0.627119  0.507692  0.788368\n",
       " 2  0.652482  0.258065  0.516667  0.470588  0.784314\n",
       " 3  0.543536  0.428571  0.467213  0.547619  0.857143\n",
       " 4  0.608069  0.407407  0.521368  0.434783  0.822898\n",
       " 5  0.643939  0.400000  0.486726  0.370787  0.805085\n",
       " 6  0.625714  0.266667  0.495413  0.434783  0.842491\n",
       " 7  0.573446  0.250000  0.485981  0.641026  0.836879\n",
       " 8  0.566138  0.208333  0.504505  0.682927  0.833948\n",
       " 9  0.591954  0.454545  0.550459  0.550000  0.846154,\n",
       "     class42   class52   class62   class67   class90\n",
       " 0  0.609589  0.191489  0.484375  0.375000  0.893543\n",
       " 1  0.605442  0.191489  0.560606  0.550000  0.866785\n",
       " 2  0.593548  0.160000  0.500000  0.500000  0.851064\n",
       " 3  0.746377  0.187500  0.471074  0.442308  0.761269\n",
       " 4  0.685065  0.250000  0.530435  0.392157  0.795848\n",
       " 5  0.580205  0.372093  0.429688  0.634615  0.818966\n",
       " 6  0.684375  0.307692  0.446281  0.392157  0.814159\n",
       " 7  0.692833  0.170213  0.495238  0.490196  0.786667\n",
       " 8  0.727891  0.113636  0.470588  0.538462  0.770017\n",
       " 9  0.698305  0.357143  0.483871  0.485294  0.814815,\n",
       "     class42   class52   class62   class67   class90\n",
       " 0  0.655617  0.214286  0.546256  0.420000  0.827141\n",
       " 1  0.647273  0.211765  0.592000  0.528000  0.825719\n",
       " 2  0.621622  0.197531  0.508197  0.484848  0.816327\n",
       " 3  0.629008  0.260870  0.469136  0.489362  0.806366\n",
       " 4  0.644275  0.309859  0.525862  0.412371  0.809147\n",
       " 5  0.610413  0.385542  0.456432  0.468085  0.811966\n",
       " 6  0.653731  0.285714  0.469565  0.412371  0.828083\n",
       " 7  0.627512  0.202532  0.490566  0.555556  0.810997\n",
       " 8  0.636905  0.147059  0.486957  0.602151  0.800709\n",
       " 9  0.640747  0.400000  0.515021  0.515625  0.830189,\n",
       "    accuracy  bal_accuracy\n",
       " 0  0.723540      0.536247\n",
       " 1  0.727190      0.586434\n",
       " 2  0.714416      0.550847\n",
       " 3  0.729927      0.563215\n",
       " 4  0.720803      0.535402\n",
       " 5  0.705292      0.533180\n",
       " 6  0.708942      0.534301\n",
       " 7  0.680657      0.551014\n",
       " 8  0.723540      0.563147\n",
       " 9  0.724453      0.589009,\n",
       "    accuracy  bal_accuracy\n",
       " 0  0.723540      0.529442\n",
       " 1  0.722628      0.580853\n",
       " 2  0.705292      0.533935\n",
       " 3  0.722628      0.555345\n",
       " 4  0.715328      0.541242\n",
       " 5  0.698905      0.546232\n",
       " 6  0.710766      0.542327\n",
       " 7  0.698905      0.526347\n",
       " 8  0.726277      0.546384\n",
       " 9  0.726277      0.596924,\n",
       "    accuracy  bal_accuracy\n",
       " 0  0.802007      0.781086\n",
       " 1  0.823905      0.738558\n",
       " 2  0.795620      0.727596\n",
       " 3  0.792883      0.730585\n",
       " 4  0.804745      0.733684\n",
       " 5  0.813869      0.699565\n",
       " 6  0.797445      0.715561\n",
       " 7  0.784672      0.750088\n",
       " 8  0.794708      0.698140\n",
       " 9  0.816606      0.740695,\n",
       "    accuracy  bal_accuracy\n",
       " 0  0.765511      0.665510\n",
       " 1  0.740876      0.659102\n",
       " 2  0.731752      0.641455\n",
       " 3  0.729015      0.605800\n",
       " 4  0.753650      0.649713\n",
       " 5  0.742701      0.611435\n",
       " 6  0.761861      0.642301\n",
       " 7  0.739051      0.647221\n",
       " 8  0.751825      0.635577,\n",
       "    accuracy  bal_accuracy\n",
       " 0  0.765511      0.665510\n",
       " 1  0.740876      0.659102\n",
       " 2  0.731752      0.641455\n",
       " 3  0.729015      0.605800\n",
       " 4  0.753650      0.649713\n",
       " 5  0.742701      0.611435\n",
       " 6  0.761861      0.642301\n",
       " 7  0.739051      0.647221\n",
       " 8  0.751825      0.635577)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ancestor_precision_df=put_in_df(ancestor_precision_results,\"ancestor\")\n",
    "ancestor_recall_df=put_in_df(ancestor_recall_results,\"ancestor\")\n",
    "ancestor_f1_df=put_in_df(ancestor_f1_val,\"ancestor\")\n",
    "\n",
    "\n",
    "parent_precision_df=put_in_df(parent_precision_results,\"parent\")\n",
    "parent_recall_df=put_in_df(parent_recall_results,\"parent\")\n",
    "parent_f1_df=put_in_df(parent_f1_val,\"parent\")\n",
    "\n",
    "target_precision_df=put_in_df(target_precision_results,\"target\")\n",
    "target_recall_df=put_in_df(target_recall_results,\"target\")\n",
    "target_f1_df=put_in_df(target_f1_val,\"target\")\n",
    "\n",
    "\n",
    "flat_precision_df=put_in_df(flat_precision_results,\"flat\")\n",
    "flat_recall_df=put_in_df(flat_recall_results,\"flat\")\n",
    "flat_f1_df=put_in_df(flat_f1_val,\"flat\")\n",
    "\n",
    "\n",
    "stack_precision_df=put_in_df(stack_precision_results,\"flat\")\n",
    "stack_recall_df=put_in_df(stack_recall_results,\"flat\")\n",
    "stack_f1_df=put_in_df(stack_f1_val,\"flat\")\n",
    "\n",
    "\n",
    "flat_acc_df = put_in_df(flat_accuracy_results,\"accuracy\")\n",
    "stack_acc_df = put_in_df(stack_accuracy_results,\"accuracy\")\n",
    "ancestor_acc_df = put_in_df(ancestor_accuracy_results,\"accuracy\")\n",
    "parent_acc_df = put_in_df(parent_accuracy_results,\"accuracy\")\n",
    "target_acc_df = put_in_df(target_accuracy_results,\"accuracy\")\n",
    "\n",
    "flat_precision_df.to_csv('C:/kv/Thesus/PLAStiCC/results/deep_flat_precision.csv')\n",
    "flat_recall_df.to_csv('C:/kv/Thesus/PLAStiCC/results/deep_flat_recall.csv')\n",
    "flat_f1_df.to_csv('C:/kv/Thesus/PLAStiCC/results/deep_flat_f1.csv')\n",
    "\n",
    "stack_precision_df.to_csv('C:/kv/Thesus/PLAStiCC/results/deep_stack_precision.csv')\n",
    "stack_recall_df.to_csv('C:/kv/Thesus/PLAStiCC/results/deep_stack_recall.csv')\n",
    "stack_f1_df.to_csv('C:/kv/Thesus/PLAStiCC/results/deep_stack_f1.csv')\n",
    "\n",
    "\n",
    "ancestor_precision_df.to_csv('C:/kv/Thesus/PLAStiCC/results/deep_ancestor_precision.csv')\n",
    "ancestor_recall_df.to_csv('C:/kv/Thesus/PLAStiCC/results/deep_ancestor_recall.csv')\n",
    "ancestor_f1_df.to_csv('C:/kv/Thesus/PLAStiCC/results/deep_ancestor_f1.csv')\n",
    "\n",
    "\n",
    "parent_precision_df.to_csv('C:/kv/Thesus/PLAStiCC/results/deep_parent_precision.csv')\n",
    "parent_recall_df.to_csv('C:/kv/Thesus/PLAStiCC/results/deep_parent_recall.csv')\n",
    "parent_f1_df.to_csv('C:/kv/Thesus/PLAStiCC/results/deep_parent_f1.csv')\n",
    "\n",
    "\n",
    "target_precision_df.to_csv('C:/kv/Thesus/PLAStiCC/results/deep_target_precision.csv')\n",
    "target_recall_df.to_csv('C:/kv/Thesus/PLAStiCC/results/deep_target_recall.csv')\n",
    "target_f1_df.to_csv('C:/kv/Thesus/PLAStiCC/results/deep_target_f1.csv')\n",
    "\n",
    "flat_acc_df.to_csv('C:/kv/Thesus/PLAStiCC/results/deep_flat_acc.csv')\n",
    "stack_acc_df.to_csv('C:/kv/Thesus/PLAStiCC/results/deep_stack_acc.csv')\n",
    "ancestor_acc_df.to_csv('C:/kv/Thesus/PLAStiCC/results/deep_ancestor_acc.csv')\n",
    "parent_acc_df.to_csv('C:/kv/Thesus/PLAStiCC/results/deep_parent_acc.csv')\n",
    "target_acc_df.to_csv('C:/kv/Thesus/PLAStiCC/results/deep_target_acc.csv')\n",
    "\n",
    "\n",
    "del stack_precision_df,stack_recall_df,stack_f1_df,flat_precision_df,flat_recall_df,flat_f1_df,ancestor_precision_df,ancestor_recall_df,ancestor_f1_df,parent_precision_df,parent_recall_df,parent_f1_df,\n",
    "target_precision_df,target_recall_df,target_f1_df,flat_acc_df,stack_acc_df,ancestor_acc_df,parent_acc_df,parent_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score,make_scorer,f1_score\n",
    "\n",
    "model = KerasClassifier(build_fn=flat_model,verbose=0)\n",
    "batch_size = [100, 500, 1000, 2000]\n",
    "epochs = [100,200,300]\n",
    "#optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "#param_grid = dict(optimizer=optimizer)\n",
    "#learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "#activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "#units = [8,10,12,14,16]\n",
    "#param_grid = dict(units=units)\n",
    "#weight_constraint = [1, 2, 3, 4, 5]\n",
    "#dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "param_grid = dict(batch_size=batch_size,epochs=epochs)\n",
    "#scoring = {'bal_accuracy': make_scorer(balanced_accuracy_score)}\n",
    "#f1_scorer = make_scorer(f1_score,average='micro',greater_is_better=True)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(flat_X_train,flat_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Best: 0.701065 using {'optimizer': 'Adam'}\n",
    "#0.666058 (0.011317) with: {'optimizer': 'SGD'}\n",
    "#0.688280 (0.012305) with: {'optimizer': 'RMSprop'}\n",
    "#0.674886 (0.008202) with: {'optimizer': 'Adagrad'}\n",
    "#0.692542 (0.007165) with: {'optimizer': 'Adadelta'}\n",
    "#0.701065 (0.007761) with: {'optimizer': 'Adam'}\n",
    "#0.686454 (0.008642) with: {'optimizer': 'Adamax'}\n",
    "#0.687671 (0.008066) with: {'optimizer': 'Nadam'}\n",
    "\n",
    "\n",
    "#Best: 0.690107 using {'learn_rate': 0.001}\n",
    "#0.690107 (0.006766) with: {'learn_rate': 0.001}\n",
    "#0.656621 (0.006501) with: {'learn_rate': 0.01}\n",
    "#0.603957 (0.028003) with: {'learn_rate': 0.1}\n",
    "#0.542770 (0.012705) with: {'learn_rate': 0.2}\n",
    "#0.512329 (0.032133) with: {'learn_rate': 0.3}\n",
    "\n",
    "\n",
    "\n",
    "#Best: 0.704110 using {'init_mode': 'normal'}\n",
    "#0.693455 (0.010607) with: {'init_mode': 'uniform'}\n",
    "#0.687976 (0.006027) with: {'init_mode': 'lecun_uniform'}\n",
    "#0.704110 (0.016856) with: {'init_mode': 'normal'}\n",
    "#0.529680 (0.009041) with: {'init_mode': 'zero'}\n",
    "#0.697717 (0.008469) with: {'init_mode': 'glorot_normal'}\n",
    "#0.695586 (0.008214) with: {'init_mode': 'glorot_uniform'}\n",
    "#0.693151 (0.012455) with: {'init_mode': 'he_normal'}\n",
    "#0.692542 (0.006073) with: {'init_mode': 'he_uniform'}\n",
    "\n",
    "#Best: 0.702892 using {'activation': 'softplus'}\n",
    "#0.654795 (0.009163) with: {'activation': 'softmax'}\n",
    "#0.702892 (0.003444) with: {'activation': 'softplus'}\n",
    "#0.698630 (0.006834) with: {'activation': 'softsign'}\n",
    "#0.689193 (0.018300) with: {'activation': 'relu'}\n",
    "#0.695282 (0.008642) with: {'activation': 'tanh'}\n",
    "#0.691020 (0.004965) with: {'activation': 'sigmoid'}\n",
    "#0.683714 (0.005496) with: {'activation': 'hard_sigmoid'}\n",
    "#0.650228 (0.001292) with: {'activation': 'linear'}\n",
    "\n",
    "\n",
    "#Best: 0.708371 using {'units': 12}\n",
    "#0.699239 (0.009442) with: {'units': 8}\n",
    "#0.698630 (0.002237) with: {'units': 10}\n",
    "#0.708371 (0.007616) with: {'units': 12}\n",
    "#0.706240 (0.006766) with: {'units': 14}\n",
    "#0.705632 (0.005792) with: {'units': 16}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
